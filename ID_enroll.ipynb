{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "For open-set identification, we follow our previous work\n",
    "\n",
    "It consists of two steps, enrollment and identification \n",
    "\n",
    "LFW, IJBC and VGG2\n",
    "\n",
    "1.0 generate the deep features/ hash codes \n",
    "1.1 enrollement according to the protocol\n",
    "1.2 identifcation \n",
    "2.0 performance evaluation\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from modules.utils import set_memory_growth, load_yaml, l2_norm\n",
    "from modules.models import ArcFaceModel, IoMFaceModelFromArFace, IoMFaceModelFromArFaceMLossHead,IoMFaceModelFromArFace2,IoMFaceModelFromArFace3,IoMFaceModelFromArFace_T,IoMFaceModelFromArFace_T1\n",
    "import tqdm\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] load ckpt from ./checkpoints/arc_res50\\e_17_b_21576.ckpt\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "# set_memory_growth()\n",
    "\n",
    "cfg = load_yaml('./configs/iom_res50_random.yaml')  # cfg = load_yaml(FLAGS.cfg_path)\n",
    "permKey = None\n",
    "if cfg['head_type'] == 'IoMHead':  #\n",
    "    # permKey = generatePermKey(cfg['embd_shape'])\n",
    "    permKey = tf.eye(cfg['embd_shape'])  # for training, we don't permutate, won't influence the performance\n",
    "\n",
    "arcmodel = ArcFaceModel(size=cfg['input_size'],\n",
    "                        embd_shape=cfg['embd_shape'],\n",
    "                        backbone_type=cfg['backbone_type'],\n",
    "                        head_type='ArcHead',\n",
    "                        training=False,\n",
    "                        cfg=cfg)\n",
    "\n",
    "ckpt_path = tf.train.latest_checkpoint('./checkpoints/arc_res50')\n",
    "if ckpt_path is not None:\n",
    "    print(\"[*] load ckpt from {}\".format(ckpt_path))\n",
    "    arcmodel.load_weights(ckpt_path)\n",
    "else:\n",
    "    print(\"[*] Cannot find ckpt from {}.\".format(ckpt_path))\n",
    "    exit()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_dir(save_path, BATCH_SIZE=128, img_ext='png'):\n",
    "    def transform_test_images(img):\n",
    "        img = tf.image.resize(img, (112, 112))\n",
    "        img = img / 255\n",
    "        return img\n",
    "\n",
    "    def get_label_withname(file_path):\n",
    "        # convert the path to a list of path components\n",
    "        parts = tf.strings.split(file_path, os.path.sep)\n",
    "        # The second to last is the class-directory\n",
    "#         wh = tf.strings.split(parts[-1], \".\")[0]\n",
    "        wh = tf.strings.split(parts[-1], \".\")[0]\n",
    "        return wh\n",
    "\n",
    "    def process_path_withname(file_path):\n",
    "        label = get_label_withname(file_path)\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = transform_test_images(img)\n",
    "        return img, label\n",
    "\n",
    "    list_gallery_ds = tf.data.Dataset.list_files(save_path + '/*/*.' + img_ext, shuffle=False)\n",
    "    labeled_gallery_ds = list_gallery_ds.map(lambda x: process_path_withname(x))\n",
    "    dataset = labeled_gallery_ds.batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "def extractFeat(dataset, model, feature_dim):\n",
    "    final_feature = np.zeros(feature_dim)\n",
    "    feats = []\n",
    "    names = []\n",
    "    n = 0\n",
    "    for image_batch, label_batch in tqdm.tqdm(dataset):\n",
    "        feature = model(image_batch)\n",
    "        for i in range(feature.shape[0]):\n",
    "            n = n + 1\n",
    "            feats.append(feature[i].numpy())\n",
    "            mylabel = str(label_batch[i].numpy().decode(\"utf-8\")+\"\")\n",
    "#             print(mylabel)\n",
    "            names.append(mylabel)\n",
    "            \n",
    "    return feats,names,n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function load_data_from_dir.<locals>.get_label_withname at 0x000001EB0401AD90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <function load_data_from_dir.<locals>.get_label_withname at 0x000001EB0401AD90>, which Python reported as:\n",
      "    def get_label_withname(file_path):\n",
      "        # convert the path to a list of path components\n",
      "        parts = tf.strings.split(file_path, os.path.sep)\n",
      "        # The second to last is the class-directory\n",
      "#         wh = tf.strings.split(parts[-1], \".\")[0]\n",
      "        wh = tf.strings.split(parts[-1], \".\")[0]\n",
      "        return wh\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [09:54,  5.71s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data_from_dir('./data/lfw_mtcnnpy_160', BATCH_SIZE=128)\n",
    "feats,names,n = extractFeat(dataset, arcmodel, 512)\n",
    "with open('lfw_feat.csv', 'w') as f: \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f) \n",
    "    write.writerows(feats) \n",
    "with open('lfw_name.txt', 'w') as outfile:\n",
    "    for i in names:\n",
    "        outfile.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"IoMface_model\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "input_image (InputLayer)            [(None, 112, 112, 3)]           0           \n",
      "________________________________________________________________________________\n",
      "arcface_model (Model)               (None, 512)                     40375680    \n",
      "________________________________________________________________________________\n",
      "perm_layer_1 (PermLayer)            (None, 512)                     0           \n",
      "________________________________________________________________________________\n",
      "IoMProjectionLayer (Model)          (None, 1024)                    525312      \n",
      "________________________________________________________________________________\n",
      "IoMHead (Model)                     (None, 512)                     0           \n",
      "================================================================================\n",
      "Total params: 40,900,992\n",
      "Trainable params: 40,842,752\n",
      "Non-trainable params: 58,240\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = cfg['m'] = 512\n",
    "q = cfg['q'] = 2\n",
    "\n",
    "# here I add the extra IoM layer and head\n",
    "if cfg['hidden_layer_remark'] == '1':\n",
    "    model = IoMFaceModelFromArFace(size=cfg['input_size'],\n",
    "                                   arcmodel=arcmodel, training=False,\n",
    "                                   permKey=permKey, cfg=cfg)\n",
    "elif cfg['hidden_layer_remark'] == '2':\n",
    "    model = IoMFaceModelFromArFace2(size=cfg['input_size'],\n",
    "                                    arcmodel=arcmodel, training=False,\n",
    "                                    permKey=permKey, cfg=cfg)\n",
    "elif cfg['hidden_layer_remark'] == '3':\n",
    "    model = IoMFaceModelFromArFace3(size=cfg['input_size'],\n",
    "                                    arcmodel=arcmodel, training=False,\n",
    "                                    permKey=permKey, cfg=cfg)\n",
    "elif cfg['hidden_layer_remark'] == 'T':  # 2 layers\n",
    "    model = IoMFaceModelFromArFace_T(size=cfg['input_size'],\n",
    "                                     arcmodel=arcmodel, training=False,\n",
    "                                     permKey=permKey, cfg=cfg)\n",
    "elif cfg['hidden_layer_remark'] == 'T1':\n",
    "    model = IoMFaceModelFromArFace_T1(size=cfg['input_size'],\n",
    "                                      arcmodel=arcmodel, training=False,\n",
    "                                      permKey=permKey, cfg=cfg)\n",
    "else:\n",
    "    model = IoMFaceModelFromArFace(size=cfg['input_size'],\n",
    "                                   arcmodel=arcmodel, training=False,\n",
    "                                   permKey=permKey, cfg=cfg)\n",
    "model.summary(line_length=80)\n",
    "cfg['embd_shape'] = m * q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [10:32,  6.08s/it]\n"
     ]
    }
   ],
   "source": [
    "feats,names,n = extractFeat(dataset, model, m)\n",
    "with open('lfw_feat_dIoM_512x2.csv', 'w') as f: \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f) \n",
    "    write.writerows(feats) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
