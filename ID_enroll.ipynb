{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "For open-set identification, we follow our previous work\n",
    "\n",
    "It consists of two steps, enrollment and identification \n",
    "\n",
    "LFW, IJBC and VGG2\n",
    "\n",
    "1.0 generate the deep features/ hash codes \n",
    "1.1 enrollement according to the protocol\n",
    "1.2 identifcation \n",
    "2.0 performance evaluation\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from modules.utils import set_memory_growth, load_yaml, l2_norm\n",
    "from modules.models import ArcFaceModel, IoMFaceModelFromArFace, IoMFaceModelFromArFaceMLossHead,IoMFaceModelFromArFace2,IoMFaceModelFromArFace3,IoMFaceModelFromArFace_T,IoMFaceModelFromArFace_T1\n",
    "import tqdm\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] load ckpt from ./checkpoints/arc_res50\\e_5_b_43044.ckpt\n",
      "Model: \"IoMface_model\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "input_image (InputLayer)            [(None, 112, 112, 3)]           0           \n",
      "________________________________________________________________________________\n",
      "arcface_model (Model)               (None, 512)                     40375680    \n",
      "________________________________________________________________________________\n",
      "perm_layer (PermLayer)              (None, 512)                     0           \n",
      "________________________________________________________________________________\n",
      "IoMProjectionLayer (Model)          (None, 2048)                    1050624     \n",
      "________________________________________________________________________________\n",
      "IoMHead (Model)                     (None, 512)                     0           \n",
      "================================================================================\n",
      "Total params: 41,426,304\n",
      "Trainable params: 41,368,064\n",
      "Non-trainable params: 58,240\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "# set_memory_growth()\n",
    "\n",
    "cfg = load_yaml('./configs/iom_res50_random.yaml')  # cfg = load_yaml(FLAGS.cfg_path)\n",
    "permKey = None\n",
    "if cfg['head_type'] == 'IoMHead':  #\n",
    "    # permKey = generatePermKey(cfg['embd_shape'])\n",
    "    permKey = tf.eye(cfg['embd_shape'])  # for training, we don't permutate, won't influence the performance\n",
    "\n",
    "arcmodel = ArcFaceModel(size=cfg['input_size'],\n",
    "                        embd_shape=cfg['embd_shape'],\n",
    "                        backbone_type=cfg['backbone_type'],\n",
    "                        head_type='ArcHead',\n",
    "                        training=False,\n",
    "                        cfg=cfg)\n",
    "\n",
    "ckpt_path = tf.train.latest_checkpoint('./checkpoints/arc_res50')\n",
    "if ckpt_path is not None:\n",
    "    print(\"[*] load ckpt from {}\".format(ckpt_path))\n",
    "    arcmodel.load_weights(ckpt_path)\n",
    "else:\n",
    "    print(\"[*] Cannot find ckpt from {}.\".format(ckpt_path))\n",
    "    exit()\n",
    "m = cfg['m'] = 512\n",
    "q = cfg['q'] = 4\n",
    "\n",
    "# here I add the extra IoM layer and head\n",
    "if cfg['hidden_layer_remark'] == '1':\n",
    "    model = IoMFaceModelFromArFace(size=cfg['input_size'],\n",
    "                                   arcmodel=arcmodel, training=False,\n",
    "                                   permKey=permKey, cfg=cfg)\n",
    "elif cfg['hidden_layer_remark'] == '2':\n",
    "    model = IoMFaceModelFromArFace2(size=cfg['input_size'],\n",
    "                                    arcmodel=arcmodel, training=False,\n",
    "                                    permKey=permKey, cfg=cfg)\n",
    "elif cfg['hidden_layer_remark'] == '3':\n",
    "    model = IoMFaceModelFromArFace3(size=cfg['input_size'],\n",
    "                                    arcmodel=arcmodel, training=False,\n",
    "                                    permKey=permKey, cfg=cfg)\n",
    "elif cfg['hidden_layer_remark'] == 'T':  # 2 layers\n",
    "    model = IoMFaceModelFromArFace_T(size=cfg['input_size'],\n",
    "                                     arcmodel=arcmodel, training=False,\n",
    "                                     permKey=permKey, cfg=cfg)\n",
    "elif cfg['hidden_layer_remark'] == 'T1':\n",
    "    model = IoMFaceModelFromArFace_T1(size=cfg['input_size'],\n",
    "                                      arcmodel=arcmodel, training=False,\n",
    "                                      permKey=permKey, cfg=cfg)\n",
    "else:\n",
    "    model = IoMFaceModelFromArFace(size=cfg['input_size'],\n",
    "                                   arcmodel=arcmodel, training=False,\n",
    "                                   permKey=permKey, cfg=cfg)\n",
    "model.summary(line_length=80)\n",
    "cfg['embd_shape'] = m * q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_dir(save_path, BATCH_SIZE=128, img_ext='png'):\n",
    "    def transform_test_images(img):\n",
    "        img = tf.image.resize(img, (112, 112))\n",
    "        img = img / 255\n",
    "        return img\n",
    "\n",
    "    def get_label_withname(file_path):\n",
    "        # convert the path to a list of path components\n",
    "        parts = tf.strings.split(file_path, os.path.sep)\n",
    "        # The second to last is the class-directory\n",
    "#         wh = tf.strings.split(parts[-1], \".\")[0]\n",
    "        wh = tf.strings.split(parts[-1], \".\")[0]\n",
    "        return wh\n",
    "\n",
    "    def process_path_withname(file_path):\n",
    "        label = get_label_withname(file_path)\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = transform_test_images(img)\n",
    "        return img, label\n",
    "\n",
    "    list_gallery_ds = tf.data.Dataset.list_files(save_path + '/*/*.' + img_ext, shuffle=False)\n",
    "    labeled_gallery_ds = list_gallery_ds.map(lambda x: process_path_withname(x))\n",
    "    dataset = labeled_gallery_ds.batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "def extractFeat(dataset, model, feature_dim):\n",
    "    final_feature = np.zeros(feature_dim)\n",
    "    feats = []\n",
    "    names = []\n",
    "    n = 0\n",
    "    for image_batch, label_batch in tqdm.tqdm(dataset):\n",
    "        feature = model(image_batch)\n",
    "        for i in range(feature.shape[0]):\n",
    "            n = n + 1\n",
    "            feats.append(feature[i].numpy())\n",
    "            mylabel = str(label_batch[i].numpy().decode(\"utf-8\")+\"\")\n",
    "            print(mylabel)\n",
    "            names.append(mylabel)\n",
    "            \n",
    "    return feats,names,n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_from_dir('./data/lfw_mtcnnpy_160', BATCH_SIZE=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:15, 15.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaron_Eckhart_0001\n",
      "Aaron_Guiel_0001\n",
      "Aaron_Patterson_0001\n",
      "Aaron_Peirsol_0001\n",
      "Aaron_Peirsol_0002\n",
      "Aaron_Peirsol_0003\n",
      "Aaron_Peirsol_0004\n",
      "Aaron_Pena_0001\n",
      "Aaron_Sorkin_0001\n",
      "Aaron_Sorkin_0002\n",
      "Aaron_Tippin_0001\n",
      "Abba_Eban_0001\n",
      "Abbas_Kiarostami_0001\n",
      "Abdel_Aziz_Al-Hakim_0001\n",
      "Abdel_Madi_Shabneh_0001\n",
      "Abdel_Nasser_Assidi_0001\n",
      "Abdel_Nasser_Assidi_0002\n",
      "Abdoulaye_Wade_0001\n",
      "Abdoulaye_Wade_0002\n",
      "Abdoulaye_Wade_0003\n",
      "Abdoulaye_Wade_0004\n",
      "Abdul_Majeed_Shobokshi_0001\n",
      "Abdul_Rahman_0001\n",
      "Abdulaziz_Kamilov_0001\n",
      "Abdullah_0001\n",
      "Abdullah_0002\n",
      "Abdullah_0003\n",
      "Abdullah_0004\n",
      "Abdullah_Ahmad_Badawi_0001\n",
      "Abdullah_al-Attiyah_0001\n",
      "Abdullah_al-Attiyah_0002\n",
      "Abdullah_al-Attiyah_0003\n",
      "Abdullah_Gul_0001\n",
      "Abdullah_Gul_0002\n",
      "Abdullah_Gul_0003\n",
      "Abdullah_Gul_0004\n",
      "Abdullah_Gul_0005\n",
      "Abdullah_Gul_0006\n",
      "Abdullah_Gul_0007\n",
      "Abdullah_Gul_0008\n",
      "Abdullah_Gul_0009\n",
      "Abdullah_Gul_0010\n",
      "Abdullah_Gul_0011\n",
      "Abdullah_Gul_0012\n",
      "Abdullah_Gul_0013\n",
      "Abdullah_Gul_0014\n",
      "Abdullah_Gul_0015\n",
      "Abdullah_Gul_0016\n",
      "Abdullah_Gul_0017\n",
      "Abdullah_Gul_0018\n",
      "Abdullah_Gul_0019\n",
      "Abdullah_Nasseef_0001\n",
      "Abdullatif_Sener_0001\n",
      "Abdullatif_Sener_0002\n",
      "Abel_Aguilar_0001\n",
      "Abel_Pacheco_0001\n",
      "Abel_Pacheco_0002\n",
      "Abel_Pacheco_0003\n",
      "Abel_Pacheco_0004\n",
      "Abid_Hamid_Mahmud_Al-Tikriti_0001\n",
      "Abid_Hamid_Mahmud_Al-Tikriti_0002\n",
      "Abid_Hamid_Mahmud_Al-Tikriti_0003\n",
      "Abner_Martinez_0001\n",
      "Abraham_Foxman_0001\n",
      "Aby_Har-Even_0001\n",
      "Adam_Ant_0001\n",
      "Adam_Freier_0001\n",
      "Adam_Herbert_0001\n",
      "Adam_Kennedy_0001\n",
      "Adam_Mair_0001\n",
      "Adam_Rich_0001\n",
      "Adam_Sandler_0001\n",
      "Adam_Sandler_0002\n",
      "Adam_Sandler_0003\n",
      "Adam_Sandler_0004\n",
      "Adam_Scott_0001\n",
      "Adam_Scott_0002\n",
      "Adel_Al-Jubeir_0001\n",
      "Adel_Al-Jubeir_0002\n",
      "Adel_Al-Jubeir_0003\n",
      "Adelina_Avila_0001\n",
      "Adisai_Bodharamik_0001\n",
      "Adolfo_Aguilar_Zinser_0001\n",
      "Adolfo_Aguilar_Zinser_0002\n",
      "Adolfo_Aguilar_Zinser_0003\n",
      "Adolfo_Rodriguez_Saa_0001\n",
      "Adolfo_Rodriguez_Saa_0002\n",
      "Adoor_Gopalakarishnan_0001\n",
      "Adrian_Annus_0001\n",
      "Adrian_Fernandez_0001\n",
      "Adrian_McPherson_0001\n",
      "Adrian_McPherson_0002\n",
      "Adrian_Murrell_0001\n",
      "Adrian_Nastase_0001\n",
      "Adrian_Nastase_0002\n",
      "Adriana_Lima_0001\n",
      "Adriana_Perez_Navarro_0001\n",
      "Adrianna_Zuzic_0001\n",
      "Adrien_Brody_0001\n",
      "Adrien_Brody_0002\n",
      "Adrien_Brody_0003\n",
      "Adrien_Brody_0004\n",
      "Adrien_Brody_0005\n",
      "Adrien_Brody_0006\n",
      "Adrien_Brody_0007\n",
      "Adrien_Brody_0008\n",
      "Adrien_Brody_0009\n",
      "Adrien_Brody_0010\n",
      "Adrien_Brody_0011\n",
      "Adrien_Brody_0012\n",
      "Afton_Smith_0001\n",
      "Agbani_Darego_0001\n",
      "Agnelo_Queiroz_0001\n",
      "Agnes_Bruckner_0001\n",
      "Ahmad_Jbarah_0001\n",
      "Ahmad_Masood_0001\n",
      "Ahmad_Masood_0002\n",
      "Ahmed_Ahmed_0001\n",
      "Ahmed_Chalabi_0001\n",
      "Ahmed_Chalabi_0002\n",
      "Ahmed_Chalabi_0003\n",
      "Ahmed_Chalabi_0004\n",
      "Ahmed_Chalabi_0005\n",
      "Ahmed_Ghazi_0001\n",
      "Ahmed_Ibrahim_Bilal_0001\n",
      "Ahmed_Lopez_0001\n",
      "Ahmed_Qureia_0001\n",
      "Ahmet_Demir_0001\n"
     ]
    }
   ],
   "source": [
    "feats,names,n = extractFeat(dataset, model, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lfw_feat.csv', 'w') as f: \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f) \n",
    "    write.writerows(feats) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lfw_name.txt', 'w') as outfile:\n",
    "    for i in names:\n",
    "        outfile.write(i+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
