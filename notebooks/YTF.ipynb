{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import modules\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from modules.evaluations import get_val_data, perform_val, perform_val_yts\n",
    "from modules.models import ArcFaceModel, IoMFaceModelFromArFace, IoMFaceModelFromArFaceMLossHead,IoMFaceModelFromArFace2,IoMFaceModelFromArFace3,IoMFaceModelFromArFace_T,IoMFaceModelFromArFace_T1\n",
    "from modules.utils import set_memory_growth, load_yaml, l2_norm\n",
    "\n",
    "# modules.utils.set_memory_growth()\n",
    "flags.DEFINE_string('cfg_path', './configs/iom_res50.yaml', 'config file path')\n",
    "flags.DEFINE_string('ckpt_epoch', '', 'config file path')\n",
    "flags.DEFINE_string('gpu', '0', 'which gpu to use')\n",
    "flags.DEFINE_string('img_path', '', 'path to input image')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "    logger = tf.get_logger()\n",
    "    logger.disabled = True\n",
    "    logger.setLevel(logging.FATAL)\n",
    "    set_memory_growth()\n",
    "\n",
    "    cfg = load_yaml('./configs/iom_res50_random.yaml')\n",
    "    permKey = None\n",
    "    if cfg['head_type'] == 'IoMHead':  #\n",
    "        # permKey = generatePermKey(cfg['embd_shape'])\n",
    "        permKey = tf.eye(cfg['embd_shape'])  # for training, we don't permutate, won't influence the performance\n",
    "\n",
    "    arcmodel = ArcFaceModel(size=cfg['input_size'],\n",
    "                            embd_shape=cfg['embd_shape'],\n",
    "                            backbone_type=cfg['backbone_type'],\n",
    "                            head_type='ArcHead',\n",
    "                            training=False,\n",
    "                            cfg=cfg)\n",
    "\n",
    "    ckpt_path = tf.train.latest_checkpoint('./checkpoints/arc_res50')\n",
    "    if ckpt_path is not None:\n",
    "        print(\"[*] load ckpt from {}\".format(ckpt_path))\n",
    "        arcmodel.load_weights(ckpt_path)\n",
    "    else:\n",
    "        print(\"[*] Cannot find ckpt from {}.\".format(ckpt_path))\n",
    "        exit()\n",
    "#     m = cfg['m'] = 512\n",
    "#     q = cfg['q'] = 8\n",
    "#     model = IoMFaceModelFromArFace(size=cfg['input_size'],\n",
    "#                                    arcmodel=arcmodel, training=False,\n",
    "#                                    permKey=permKey, cfg=cfg)\n",
    "    model = arcmodel\n",
    "    model.summary(line_length=80)\n",
    "    model.layers[0].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "link = \"https://www.cs.tau.ac.il/~wolf/ytfaces/splits.txt\"\n",
    "\n",
    "file = urllib.request.urlopen(link)\n",
    "listmy = []\n",
    "for line in file:\n",
    "  decoded_line = line.decode(\"utf-8\")\n",
    "  listmy.append(decoded_line.split(\",\"))\n",
    "\n",
    "\n",
    "# f = urllib.urlopen(link)\n",
    "# myfile = f.read()\n",
    "# print(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listmy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_dir(save_path,BATCH_SIZE=128,subset='Sadie_Frost/1',img_ext='jpg'):\n",
    "    def transform_test_images(img):\n",
    "        img = tf.image.resize(img, (112, 112))\n",
    "        img = img / 255\n",
    "        return img\n",
    "\n",
    "    def get_label_withname( file_path):\n",
    "        # convert the path to a list of path components\n",
    "        parts = tf.strings.split(file_path, os.path.sep)\n",
    "        # The second to last is the class-directory\n",
    "        wh = parts[-2]\n",
    "        return wh\n",
    "    def process_path_withname(file_path):\n",
    "      label = get_label_withname(file_path)\n",
    "      img = tf.io.read_file(file_path)\n",
    "      img = tf.image.decode_jpeg(img, channels=3)\n",
    "      img = transform_test_images(img)\n",
    "      return img, label\n",
    "#     list_gallery_ds = tf.data.Dataset.list_files(save_path +'/'+subset+'/*.'+img_ext).shuffle(100).take(5)\n",
    "    list_gallery_ds = tf.data.Dataset.list_files(save_path +'/'+subset+'/*.'+img_ext)\n",
    "    labeled_gallery_ds = list_gallery_ds.map(lambda x:process_path_withname(x), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = labeled_gallery_ds.batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "# dataset = load_data_from_dir('./data/test_dataset/aligned_images_DB_YTF/160x160',subset='Sadie_Frost/1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliza_Dushku Choi_Sung-hong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def extractFeat(dataset, model,feature_dim=512):\n",
    "        final_feature = np.zeros(feature_dim)\n",
    "        feats = []\n",
    "        names = []\n",
    "        n = 0\n",
    "        for image_batch, label_batch in dataset:\n",
    "            feature = model(image_batch)\n",
    "            for i in range(feature.shape[0]):\n",
    "                n = n + 1\n",
    "                feats.append(feature[i])\n",
    "                mylabel = label_batch[i].numpy()\n",
    "                names.append(mylabel)\n",
    "                if feature[i] is not None:\n",
    "                    final_feature += feature[i] / np.linalg.norm(feature[i], ord=2)\n",
    "#         print(f\"[*] finanly we have {n} extracted samples features\"\n",
    "        final_feature /= np.linalg.norm(final_feature, ord=2)\n",
    "        return final_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics as metrics\n",
    "import tqdm\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def eucliden_dist(embeddings1, embeddings2):\n",
    "    diff = np.subtract(embeddings1, embeddings2)\n",
    "    dist = np.sum(np.square(diff), 1)\n",
    "    return dist\n",
    "\n",
    "\n",
    "scores = []\n",
    "issames=[]\n",
    "for i in tqdm.tqdm(range(1, 5001)):\n",
    "    first_name = listmy[i][2].strip()\n",
    "    second_name = listmy[i][3].strip()\n",
    "    issame = int(listmy[i][4].strip())\n",
    "    try:\n",
    "        dataset_1 = load_data_from_dir('./data/test_dataset/aligned_images_DB_YTF/160x160',subset=first_name)\n",
    "        dataset_2 = load_data_from_dir('./data/test_dataset/aligned_images_DB_YTF/160x160',subset=second_name)\n",
    "    \n",
    "    except Exception:\n",
    "        print('[*]',first_name,second_name,'failed')\n",
    "        continue\n",
    "    feats1 = extractFeat(dataset_1, model)\n",
    "    feats2 = extractFeat(dataset_2, model)\n",
    "#     dist = sklearn.metrics.pairwise_distances(feats1, feats2, metric='hamming')\n",
    "    score = distance.euclidean(feats1, feats2)\n",
    "        # dist = distance.hamming(embeddings1, embeddings2)\n",
    "#     dist = tf.linalg.diag_part(dist)\n",
    "#     dist = dist.numpy()\n",
    "#     score = np.average(dist)\n",
    "    print('issame',issame,'score',score)\n",
    "    scores.append(score)\n",
    "    issames.append(issame)\n",
    "\n",
    "\n",
    "    \n",
    "# https://github.com/huangyangyu/SeqFace/blob/master/code/YTF/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    nrof_pairs = len(issames)\n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    nrof_folds=10\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "    tprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "    accuracy = np.zeros((nrof_folds))\n",
    "    best_thresholds = np.zeros((nrof_folds))\n",
    "    indices = np.arange(nrof_pairs)\n",
    "    print(nrof_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame),\n",
    "                               np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n",
    "    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n",
    "    acc = float(tp + tn) / dist.size\n",
    "    return tpr, fpr, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    from scipy.optimize import brentq\n",
    "    from scipy import interpolate\n",
    "\n",
    "    dist = np.array(scores)\n",
    "    actual_issame = np.array(issames)\n",
    "    indices = np.arange(nrof_pairs)\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        # Find the best threshold for the fold\n",
    "        acc_train = np.zeros((nrof_thresholds))\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, _, acc_train[threshold_idx] = calculate_accuracy(\n",
    "                    threshold, dist[train_set], actual_issame[train_set])\n",
    "        best_threshold_index = np.argmax(acc_train)\n",
    "\n",
    "        best_thresholds[fold_idx] = thresholds[best_threshold_index]\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = \\\n",
    "                calculate_accuracy(threshold,\n",
    "                                   dist[test_set],\n",
    "                                   actual_issame[test_set])\n",
    "        _, _, accuracy[fold_idx] = calculate_accuracy(\n",
    "            thresholds[best_threshold_index],\n",
    "            dist[test_set],\n",
    "            actual_issame[test_set])\n",
    "\n",
    "    tpr = np.mean(tprs, 0)\n",
    "    fpr = np.mean(fprs, 0)\n",
    "\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    # print('Area Under Curve (AUC): %1.3f' % auc)\n",
    "    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    print('Equal Error Rate (EER): %1.3f' % eer)# 512 8 10.9% Original 9.8%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
